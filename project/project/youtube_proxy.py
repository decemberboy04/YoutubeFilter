#!/usr/bin/env python3
"""
YouTubeæ•æ„Ÿå†…å®¹å®¡æŸ¥ä»£ç†
é›†æˆæ•æ„Ÿè¯æ£€æµ‹å’Œå†…å®¹è¿‡æ»¤åŠŸèƒ½
"""
import mitmproxy.http
from mitmproxy import ctx
import json
import datetime
import re
import os
from urllib.parse import urlparse, parse_qs, unquote

# å¯¼å…¥æ•æ„Ÿè¯å®¡æŸ¥æ¨¡å—
from sensitive_filter import check_and_filter_data, get_filter_stats, set_sensitivity_threshold, \
    get_sensitivity_threshold, sensitive_filter


class ContentExtractor:
    def __init__(self):
        self.content_fields = {
            'title', 'description', 'content', 'text',
            'message', 'name', 'query', 'comment',
            'caption', 'transcript', 'summary', 'label',
            'snippet', 'display', 'simpleText', 'accessibility',
            'shortDescription', 'ownerChannelName', 'qualityLabel'
        }

    def extract_content(self, text):
        """ä½¿ç”¨JSONè§£æå’Œæ­£åˆ™è¡¨è¾¾å¼ç»“åˆæå–å†…å®¹"""
        if not text:
            return {}

        extracted_content = {}

        try:
            # é¦–å…ˆå°è¯•è§£æä¸ºJSON
            data = json.loads(text)
            ctx.log.info("âœ… æˆåŠŸè§£æJSONï¼Œå¼€å§‹æå–å†…å®¹å­—æ®µ...")
            extracted_content = self._extract_from_json(data)
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯JSONï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–
            ctx.log.info("å“åº”ä¸æ˜¯JSONæ ¼å¼ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å†…å®¹")
            extracted_content = self._extract_with_regex(text)
        except Exception as e:
            ctx.log.error(f"JSONè§£æé”™è¯¯: {e}")
            extracted_content = self._extract_with_regex(text)

        ctx.log.info(f"ğŸ“Š æ€»å…±æå–åˆ° {len(extracted_content)} ä¸ªå†…å®¹å­—æ®µ")
        return extracted_content

    def _extract_from_json(self, data, path=""):
        """é€’å½’ä»JSONä¸­æå–å†…å®¹å­—æ®µ"""
        result = {}

        if isinstance(data, dict):
            for key, value in data.items():
                current_path = f"{path}.{key}" if path else key

                # æ£€æŸ¥æ˜¯å¦æ˜¯å†…å®¹å­—æ®µ
                if (isinstance(key, str) and
                        any(pattern in key.lower() for pattern in self.content_fields) and
                        isinstance(value, str) and value.strip()):

                    if key not in result:
                        result[key] = []
                    result[key].append(value)
                    ctx.log.info(f"ğŸ“ æå–å­—æ®µ: {key} = {value[:50]}...")

                # é€’å½’æ£€æŸ¥åµŒå¥—ç»“æ„
                if isinstance(value, (dict, list)):
                    nested_result = self._extract_from_json(value, current_path)
                    # åˆå¹¶åµŒå¥—ç»“æœ
                    for nested_key, nested_values in nested_result.items():
                        if nested_key not in result:
                            result[nested_key] = []
                        result[nested_key].extend(nested_values)

        elif isinstance(data, list):
            for i, item in enumerate(data):
                current_path = f"{path}[{i}]" if path else f"[{i}]"
                if isinstance(item, (dict, list)):
                    nested_result = self._extract_from_json(item, current_path)
                    # åˆå¹¶åµŒå¥—ç»“æœ
                    for nested_key, nested_values in nested_result.items():
                        if nested_key not in result:
                            result[nested_key] = []
                        result[nested_key].extend(nested_values)

        return result

    def _extract_with_regex(self, text):
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å†…å®¹"""
        extracted_content = {}
        patterns = {
            'title': r'"title"\s*:\s*"([^"]+)"',
            'description': r'"description"\s*:\s*"([^"]+)"',
            'content': r'"content"\s*:\s*"([^"]+)"',
            'text': r'"text"\s*:\s*"([^"]+)"',
            'message': r'"message"\s*:\s*"([^"]+)"',
            'name': r'"name"\s*:\s*"([^"]+)"',
            'query': r'q=([^&]+)',
        }

        for content_type, pattern in patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                decoded_matches = []
                for match in matches:
                    try:
                        # å¤„ç†Unicodeè½¬ä¹‰åºåˆ—
                        decoded = bytes(match, 'utf-8').decode('unicode_escape')
                        if decoded not in decoded_matches:
                            decoded_matches.append(decoded)
                    except:
                        if match not in decoded_matches:
                            decoded_matches.append(match)

                if decoded_matches:
                    extracted_content[content_type] = decoded_matches
                    ctx.log.debug(f"ğŸ“ æ­£åˆ™æå–: {content_type} = {decoded_matches[0][:50]}...")

        return extracted_content

    def extract_video_info(self, url, headers):
        """æå–è§†é¢‘ä¿¡æ¯"""
        info = {}

        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)

        if 'v' in query_params:
            info['video_id'] = query_params['v'][0]
        if 'docid' in query_params:
            info['video_id'] = query_params['docid'][0]

        if 'x-youtube-client-name' in headers:
            info['client_name'] = headers['x-youtube-client-name']
        if 'x-youtube-client-version' in headers:
            info['client_version'] = headers['x-youtube-client-version']

        return info


class YouTubeContentCapture:
    def __init__(self):
        self.num_requests = 0
        self.output_file = "sensitive_content_analysis.txt"
        self.filtered_file = "filtered.txt"
        self.config_file = "proxy_config.json"
        self.extractor = ContentExtractor()
        self.stats = {
            "total_requests": 0,
            "youtube_requests": 0,
            "sensitive_blocks": 0,
            "start_time": datetime.datetime.now().isoformat()
        }

        # åŠ è½½é…ç½®
        self._load_config()

        # åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶
        with open(self.output_file, 'w', encoding='utf-8') as f:
            f.write("YouTubeæ•æ„Ÿå†…å®¹å®¡æŸ¥æ—¥å¿—\n")
            f.write("=" * 50 + "\n")
            f.write(f"å¼€å§‹æ—¶é—´: {self.stats['start_time']}\n\n")
            f.write(f"å®¡æŸ¥ç­–ç•¥: æ•æ„Ÿè¯åŒ¹é…æ¬¡æ•° >= {get_sensitivity_threshold()} æ—¶æ‹¦æˆªå†…å®¹\n")
            f.write("=" * 50 + "\n\n")

        # åˆå§‹åŒ–è¢«æ‹¦æˆªå†…å®¹æ–‡ä»¶
        with open(self.filtered_file, 'w', encoding='utf-8') as f:
            f.write("è¢«æ‹¦æˆªå†…å®¹è®°å½•\n")
            f.write("=" * 50 + "\n")
            f.write(f"å¼€å§‹æ—¶é—´: {self.stats['start_time']}\n\n")
            f.write(f"æ•æ„Ÿåº¦é˜ˆå€¼: {get_sensitivity_threshold()}\n")
            f.write("æ ¼å¼: {æ—¶é—´æˆ³, URL, åŒ¹é…æ¬¡æ•°, æ•æ„Ÿè¯è¯¦æƒ…}\n")
            f.write("=" * 50 + "\n\n")

        ctx.log.info(f"âœ… ä»£ç†å¯åŠ¨å®Œæˆï¼Œå½“å‰æ•æ„Ÿåº¦é˜ˆå€¼: {get_sensitivity_threshold()}")

    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "sensitivity_threshold": 3,
            "log_level": "info"
        }

        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    threshold = config.get("sensitivity_threshold", 3)
                    set_sensitivity_threshold(threshold)
                    ctx.log.info(f"ğŸ“‹ ä»é…ç½®æ–‡ä»¶åŠ è½½æ•æ„Ÿåº¦é˜ˆå€¼: {threshold}")
            else:
                # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
                with open(self.config_file, 'w', encoding='utf-8') as f:
                    json.dump(default_config, f, indent=2)
                set_sensitivity_threshold(3)
                ctx.log.info("ğŸ“‹ åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶")

        except Exception as e:
            ctx.log.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            set_sensitivity_threshold(3)

    def _save_config(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            config = {
                "sensitivity_threshold": get_sensitivity_threshold(),
                "log_level": "info"
            }
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2)
        except Exception as e:
            ctx.log.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

    def _write_to_file(self, data, filename=None):
        """å†™å…¥æ•°æ®åˆ°æ–‡ä»¶"""
        if filename is None:
            filename = self.output_file

        try:
            with open(filename, 'a', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                f.write(",\n")
        except Exception as e:
            ctx.log.error(f"å†™å…¥æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")

    def _log_filtered_content(self, flow, checked_data, data_type):
        """è®°å½•è¢«æ‹¦æˆªçš„å†…å®¹åˆ°å•ç‹¬æ–‡ä»¶"""
        sensitive_check = checked_data.get("sensitive_check", {})

        # æ„å»ºæ‹¦æˆªè®°å½•
        filtered_record = {
            "timestamp": datetime.datetime.now().isoformat(),
            "type": data_type,
            "url": flow.request.url if hasattr(flow, 'request') else "N/A",
            "host": checked_data.get("host", "N/A"),
            "match_count": sensitive_check.get("total_matches", 0),
            "is_sensitive": sensitive_check.get("is_sensitive", False),
            "sensitive_matches": checked_data.get("sensitive_matches", []),
            "action_taken": sensitive_check.get("action_taken", "none"),
            "threshold": get_sensitivity_threshold()
        }

        # å¦‚æœæ˜¯å“åº”ï¼Œæ·»åŠ çŠ¶æ€ç ä¿¡æ¯
        if data_type == "response" and hasattr(flow, 'response'):
            filtered_record["status_code"] = flow.response.status_code

        # å†™å…¥è¢«æ‹¦æˆªå†…å®¹æ–‡ä»¶
        self._write_to_file(filtered_record, self.filtered_file)

        # è®°å½•æ—¥å¿—
        ctx.log.warn(f"ğŸ“‹ è®°å½•è¢«æ‹¦æˆªå†…å®¹: {flow.request.url} - åŒ¹é…æ¬¡æ•°: {filtered_record['match_count']}")

    def _parse_request(self, flow):
        """è§£æHTTPè¯·æ±‚"""
        request = flow.request

        request_info = {
            "timestamp": datetime.datetime.now().isoformat(),
            "type": "request",
            "method": request.method,
            "url": request.url,
            "host": request.host,
            "path": urlparse(request.url).path,
            "is_youtube": "youtube.com" in request.host or "youtu.be" in request.host
        }

        video_info = self.extractor.extract_video_info(request.url, dict(request.headers))
        if video_info:
            request_info["video_info"] = video_info

        if request.text:
            extracted_content = self.extractor.extract_content(request.text)
            if extracted_content:
                request_info["extracted_content"] = extracted_content

        return request_info

    def _parse_response(self, flow):
        """è§£æHTTPå“åº”"""
        response = flow.response

        response_info = {
            "timestamp": datetime.datetime.now().isoformat(),
            "type": "response",
            "status_code": response.status_code,
            "url": flow.request.url,
            "host": flow.request.host,
            "is_youtube": "youtube.com" in flow.request.host or "youtu.be" in flow.request.host
        }

        if response.text:
            extracted_content = self.extractor.extract_content(response.text)
            if extracted_content:
                response_info["extracted_content"] = extracted_content

        return response_info

    def request(self, flow: mitmproxy.http.HTTPFlow):
        flow.request.headers.pop("If-Modified-Since", None)
        flow.request.headers.pop("If-None-Match", None)

        self.num_requests += 1
        self.stats["total_requests"] += 1

        is_youtube = "youtube.com" in flow.request.host or "youtu.be" in flow.request.host
        if is_youtube:
            self.stats["youtube_requests"] += 1
            ctx.log.info(f"ğŸ¥ YouTubeè¯·æ±‚ #{self.num_requests}: {flow.request.url}")

        request_info = self._parse_request(flow)

        # è¿›è¡Œæ•æ„Ÿè¯å®¡æŸ¥
        checked_request = check_and_filter_data(request_info)
        self._write_to_file(checked_request)

        # è®°å½•æ•æ„Ÿè¯·æ±‚åˆ°è¢«æ‹¦æˆªæ–‡ä»¶
        if checked_request.get("sensitive_check", {}).get("is_sensitive", False):
            self._log_filtered_content(flow, checked_request, "request")
            ctx.log.warn(f"ğŸš¨ æ•æ„Ÿè¯·æ±‚æ‹¦æˆª: {flow.request.url}")

    def response(self, flow: mitmproxy.http.HTTPFlow):
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯YouTubeæµé‡
        is_youtube = "youtube.com" in flow.request.host or "youtu.be" in flow.request.host
        if not is_youtube:
            return

        # ==================== æ ¸å¿ƒä¿®æ”¹ç‚¹ ====================
        # ä¸»åŠ¨ä¸ºæ‰€æœ‰YouTube APIå“åº”æ·»åŠ ç¦æ­¢ç¼“å­˜çš„å¤´ä¿¡æ¯
        # è¿™æ˜¯è§£å†³åˆ·æ–°é—®é¢˜çš„å…³é”®ï¼šç¡®ä¿æµè§ˆå™¨ä»ä¸ç¼“å­˜APIæ•°æ®
        # è¿™æ ·æ¯æ¬¡åˆ·æ–°éƒ½ä¼šç»è¿‡æˆ‘ä»¬çš„ä»£ç†è¿›è¡Œè¿‡æ»¤æ£€æŸ¥
        if "youtubei/v1/" in flow.request.path:
            flow.response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate"
            flow.response.headers["Pragma"] = "no-cache"
            flow.response.headers["Expires"] = "0"
            ctx.log.info(f"ğŸ“‹ å·²ä¸ºAPIå“åº” {flow.request.url} å¼ºåˆ¶è®¾ç½®æ— ç¼“å­˜å¤´")
        # ====================================================

        ctx.log.info(f"ğŸ” åˆ†æYouTubeå“åº”: {flow.request.url}")

        response_info = self._parse_response(flow)

        # æ‰“å°æå–çš„å†…å®¹ç”¨äºè°ƒè¯•
        if response_info.get("extracted_content"):
            ctx.log.info(f"ğŸ“‹ æå–çš„å†…å®¹å­—æ®µ: {list(response_info['extracted_content'].keys())}")
            for field, values in response_info["extracted_content"].items():
                for i, value in enumerate(values[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªå€¼
                    ctx.log.info(f"   {field}[{i}]: {value[:100]}...")  # æˆªæ–­é•¿æ–‡æœ¬

        # è¿›è¡Œæ•æ„Ÿè¯å®¡æŸ¥
        checked_response = check_and_filter_data(response_info)

        # æ‰“å°å®¡æŸ¥ç»“æœ
        match_count = checked_response.get("sensitive_check", {}).get("total_matches", 0)
        threshold = get_sensitivity_threshold()
        ctx.log.info(f"ğŸ¯ æ•æ„Ÿè¯åŒ¹é…æ¬¡æ•°: {match_count} (é˜ˆå€¼: {threshold})")

        if match_count > 0:
            ctx.log.info(f"ğŸ” åŒ¹é…è¯¦æƒ…: {checked_response.get('sensitive_matches', [])}")

        self._write_to_file(checked_response)

        # è®°å½•æ‰€æœ‰å“åº”åˆ°è¢«æ‹¦æˆªæ–‡ä»¶ï¼ˆæ— è®ºæ˜¯å¦æ•æ„Ÿï¼‰
        if checked_response.get("sensitive_check", {}).get("total_matches", 0) > 0:
            self._log_filtered_content(flow, checked_response, "response")

        # å¦‚æœå“åº”å†…å®¹æ•æ„Ÿï¼Œè¿›è¡Œæ‹¦æˆªå¤„ç†
        if checked_response.get("sensitive_check", {}).get("is_sensitive", False):
            self.stats["sensitive_blocks"] += 1

            ctx.log.warn(f"ğŸš¨ æ‹¦æˆªæ•æ„Ÿå“åº”: {flow.request.url}")
            ctx.log.warn(f"   åŒ¹é…æ¬¡æ•°: {checked_response['sensitive_check']['total_matches']}")
            ctx.log.warn(f"   å½“å‰é˜ˆå€¼: {threshold}")

            # å°è¯•å°†å“åº”æ–‡æœ¬ä½œä¸ºJSONè¿›è¡Œç»†ç²’åº¦è¿‡æ»¤
            if flow.response.text:
                try:
                    # 1. å°†åŸå§‹å“åº”æ–‡æœ¬è§£æä¸ºPythonå­—å…¸
                    original_data = json.loads(flow.response.text)

                    # 2. ä½¿ç”¨ sensitive_filter.py ä¸­çš„å‡½æ•°æ›¿æ¢æ•æ„Ÿå†…å®¹
                    #    è¿™ä¸ªå‡½æ•°ä¼šé€’å½’éå†æ•°æ®ï¼Œå°†åŒ…å«æ•æ„Ÿè¯çš„å­—ç¬¦ä¸²æ›¿æ¢æ‰
                    modified_data = sensitive_filter.replace_sensitive_content(original_data)

                    # 3. å°†ä¿®æ”¹åçš„å­—å…¸è½¬æ¢å›JSONå­—ç¬¦ä¸²ï¼Œå¹¶è®¾ç½®ä¸ºæ–°çš„å“åº”ä½“
                    flow.response.text = json.dumps(modified_data, ensure_ascii=False)
                    flow.response.headers["X-Content-Filtered"] = "true"  # æ·»åŠ ä¸€ä¸ªå¤´ï¼Œè¡¨ç¤ºå†…å®¹å·²è¢«è¿‡æ»¤

                    ctx.log.info(f"âœ… æˆåŠŸè¿‡æ»¤å“åº”ä¸­çš„æ•æ„Ÿå†…å®¹ï¼Œé¡µé¢å¯æ­£å¸¸åŠ è½½ã€‚")

                except json.JSONDecodeError:
                    # å¦‚æœå“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œæ— æ³•è¿›è¡Œç»†ç²’åº¦è¿‡æ»¤ï¼Œæ‰§è¡Œå›é€€ç­–ç•¥
                    ctx.log.warn(f"âš ï¸ å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œæ— æ³•è¿›è¡Œå†…å®¹æ›¿æ¢ã€‚è¿”å›é€šç”¨è¿‡æ»¤æ¶ˆæ¯ã€‚")
                    flow.response.text = "Content filtered due to sensitive material (non-JSON response)"
                    flow.response.status_code = 403
                except Exception as e:
                    ctx.log.error(f"å“åº”è¿‡æ»¤æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                    flow.response.text = "Error during content filtering"
                    flow.response.status_code = 500

        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        if checked_response["is_youtube"]:
            content_count = len(checked_response.get("extracted_content", {}))
            ctx.log.info(f"ğŸ“º YouTubeå“åº”: çŠ¶æ€ç  {flow.response.status_code}, å†…å®¹å­—æ®µ {content_count}")

    def done(self):
        """mitmproxyç»“æŸæ—¶è°ƒç”¨"""
        stats = get_filter_stats()
        self.stats.update(stats)

        # æ›´æ–°ä¸»è¾“å‡ºæ–‡ä»¶
        with open(self.output_file, 'a', encoding='utf-8') as f:
            f.write("\n" + "=" * 50 + "\n")
            f.write("å®¡æŸ¥ç»Ÿè®¡ä¿¡æ¯:\n")
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
            f.write("\n" + "=" * 50 + "\n")

        # æ›´æ–°è¢«æ‹¦æˆªå†…å®¹æ–‡ä»¶
        with open(self.filtered_file, 'a', encoding='utf-8') as f:
            f.write("\n" + "=" * 50 + "\n")
            f.write("æ‹¦æˆªç»Ÿè®¡:\n")
            f.write(f"æ€»æ‹¦æˆªæ¬¡æ•°: {self.stats['sensitive_blocks']}\n")
            f.write(f"æ€»è¯·æ±‚æ•°: {self.stats['total_requests']}\n")
            f.write(f"YouTubeè¯·æ±‚æ•°: {self.stats['youtube_requests']}\n")
            f.write(f"æœ€ç»ˆæ•æ„Ÿåº¦é˜ˆå€¼: {get_sensitivity_threshold()}\n")
            f.write("=" * 50 + "\n")


# æ·»åŠ æ•è·å™¨åˆ°mitmproxy
addons = [YouTubeContentCapture()]


# æ·»åŠ å‘½ä»¤è¡Œå‚æ•°å¤„ç†
def configure(updated):
    """é…ç½®æ›´æ–°æ—¶çš„å›è°ƒå‡½æ•°"""
    ctx.log.info("é…ç½®å·²æ›´æ–°")