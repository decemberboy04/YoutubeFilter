#!/usr/bin/env python3
"""
YouTubeæ•æ„Ÿå†…å®¹å®¡æŸ¥æ•è·è„šæœ¬
ä¸“æ³¨äºæå–å¯¹æ•æ„Ÿè¯åŒ¹é…æœ‰ç”¨çš„ä¿¡æ¯
"""
import mitmproxy.http
from mitmproxy import ctx
import json
import datetime
import re
from urllib.parse import urlparse, parse_qs, unquote

class ContentExtractor:
    def __init__(self):
        self.sensitive_patterns = {
            'title': r'"title":"([^"]+)"',
            'description': r'"description":"([^"]+)"',
            'content': r'"content":"([^"]+)"',
            'text': r'"text":"([^"]+)"',
            'message': r'"message":"([^"]+)"',
            'name': r'"name":"([^"]+)"',
            'query': r'q=([^&]+)',  # æœç´¢æŸ¥è¯¢
        }
    
    def extract_sensitive_content(self, text):
        """æå–å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯çš„æ–‡æœ¬å†…å®¹"""
        if not text:
            return {}
        
        extracted_content = {}
        
        for content_type, pattern in self.sensitive_patterns.items():
            matches = re.findall(pattern, text)
            if matches:
                # URLè§£ç å¹¶å»é‡
                decoded_matches = []
                for match in matches:
                    try:
                        decoded = unquote(match)
                        if decoded not in decoded_matches:
                            decoded_matches.append(decoded)
                    except:
                        if match not in decoded_matches:
                            decoded_matches.append(match)
                
                if decoded_matches:
                    extracted_content[content_type] = decoded_matches
        
        return extracted_content
    
    def extract_video_info(self, url, headers):
        """ä»URLå’Œå¤´ä¿¡æ¯ä¸­æå–è§†é¢‘ç›¸å…³ä¿¡æ¯"""
        info = {}
        
        # ä»URLæå–è§†é¢‘ID
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)
        
        if 'v' in query_params:
            info['video_id'] = query_params['v'][0]
        if 'docid' in query_params:
            info['video_id'] = query_params['docid'][0]
        
        # ä»headersæå–ç”¨æˆ·ä¿¡æ¯
        if 'x-youtube-client-name' in headers:
            info['client_name'] = headers['x-youtube-client-name']
        if 'x-youtube-client-version' in headers:
            info['client_version'] = headers['x-youtube-client-version']
        
        return info

class YouTubeContentCapture:
    def __init__(self):
        self.num_requests = 0
        self.output_file = "sensitive_content_analysis.txt"
        self.extractor = ContentExtractor()
        
        with open(self.output_file, 'w', encoding='utf-8') as f:
            f.write("YouTubeæ•æ„Ÿå†…å®¹åˆ†ææ—¥å¿—\n")
            f.write("=" * 50 + "\n")
            f.write(f"å¼€å§‹æ—¶é—´: {datetime.datetime.now()}\n\n")
            f.write("é‡ç‚¹å…³æ³¨ä»¥ä¸‹å†…å®¹è¿›è¡Œæ•æ„Ÿè¯åŒ¹é…ï¼š\n")
            f.write("1. è§†é¢‘æ ‡é¢˜å’Œæè¿°\n")
            f.write("2. è¯„è®ºå†…å®¹\n") 
            f.write("3. æœç´¢æŸ¥è¯¢\n")
            f.write("4. ç”¨æˆ·æ¶ˆæ¯å’Œæ–‡æœ¬å†…å®¹\n")
            f.write("=" * 50 + "\n\n")

    def _parse_request(self, flow):
        """è§£æHTTPè¯·æ±‚"""
        request = flow.request
        parsed_url = urlparse(request.url)
        
        request_info = {
            "timestamp": datetime.datetime.now().isoformat(),
            "type": "request",
            "method": request.method,
            "url": request.url,
            "host": request.host,
            "path": parsed_url.path,
            "is_youtube": "youtube.com" in request.host or "youtu.be" in request.host
        }
        
        # æå–è§†é¢‘ä¿¡æ¯
        video_info = self.extractor.extract_video_info(request.url, dict(request.headers))
        if video_info:
            request_info["video_info"] = video_info
        
        # æå–æ•æ„Ÿå†…å®¹
        if request.text:
            sensitive_content = self.extractor.extract_sensitive_content(request.text)
            if sensitive_content:
                request_info["sensitive_content"] = sensitive_content
        
        return request_info

    def _parse_response(self, flow):
        """è§£æHTTPå“åº”"""
        response = flow.response
        
        response_info = {
            "timestamp": datetime.datetime.now().isoformat(),
            "type": "response", 
            "status_code": response.status_code,
            "url": flow.request.url,
            "is_youtube": "youtube.com" in flow.request.host or "youtu.be" in flow.request.host
        }
        
        # æå–æ•æ„Ÿå†…å®¹
        if response.text:
            sensitive_content = self.extractor.extract_sensitive_content(response.text)
            if sensitive_content:
                response_info["sensitive_content"] = sensitive_content
                
                # è®°å½•æ‰¾åˆ°çš„æ•æ„Ÿå†…å®¹
                ctx.log.info(f"ğŸ“ å‘ç° {len(sensitive_content)} ç±»å¯å®¡æŸ¥å†…å®¹")
                for content_type, contents in sensitive_content.items():
                    ctx.log.info(f"   - {content_type}: {len(contents)} æ¡")
        
        return response_info

    def _write_to_file(self, data):
        """åªå†™å…¥åŒ…å«æ•æ„Ÿå†…å®¹çš„æ•°æ®"""
        try:
            # åªæœ‰å½“åŒ…å«æ•æ„Ÿå†…å®¹æˆ–è§†é¢‘ä¿¡æ¯æ—¶æ‰å†™å…¥
            if data.get("sensitive_content") or data.get("video_info"):
                with open(self.output_file, 'a', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    f.write(",\n")
        except Exception as e:
            ctx.log.error(f"å†™å…¥æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    def request(self, flow: mitmproxy.http.HTTPFlow):
        self.num_requests += 1
        
        is_youtube = "youtube.com" in flow.request.host or "youtu.be" in flow.request.host
        if is_youtube:
            ctx.log.info(f"ğŸ¥ YouTubeè¯·æ±‚ #{self.num_requests}: {flow.request.url}")
        
        request_info = self._parse_request(flow)
        self._write_to_file(request_info)

    def response(self, flow: mitmproxy.http.HTTPFlow):
        response_info = self._parse_response(flow)
        self._write_to_file(response_info)
        
        if response_info["is_youtube"] and response_info.get("sensitive_content"):
            content_count = sum(len(v) for v in response_info["sensitive_content"].values())
            ctx.log.info(f"ğŸ“º å‘ç° {content_count} æ¡å¯å®¡æŸ¥å†…å®¹")

# æ·»åŠ æ•è·å™¨åˆ°mitmproxy
addons = [YouTubeContentCapture()]