#!/usr/bin/env python3
"""
æ•æ„Ÿè¯å®¡æŸ¥æ¨¡å—
ä½¿ç”¨Aho-Corasickç®—æ³•è¿›è¡Œé«˜æ•ˆæ¨¡å¼åŒ¹é…
"""
import ahocorasick
import os
import re
from typing import Dict, List, Any

# å…¨å±€æ•æ„Ÿåº¦é˜ˆå€¼
SENSITIVITY_THRESHOLD = 3

def set_sensitivity_threshold(threshold):
    """è®¾ç½®æ•æ„Ÿåº¦é˜ˆå€¼"""
    global SENSITIVITY_THRESHOLD
    if isinstance(threshold, int) and threshold >= 0:
        SENSITIVITY_THRESHOLD = threshold
        print(f"âœ… æ•æ„Ÿåº¦é˜ˆå€¼å·²è®¾ç½®ä¸º: {threshold}")
    else:
        print(f"âŒ æ— æ•ˆçš„é˜ˆå€¼: {threshold}ï¼Œå¿…é¡»æ˜¯éè´Ÿæ•´æ•°")

def get_sensitivity_threshold():
    """è·å–å½“å‰æ•æ„Ÿåº¦é˜ˆå€¼"""
    return SENSITIVITY_THRESHOLD

class SensitiveFilter:
    def __init__(self, word_file_path="sensitive_words.txt"):
        """
        åˆå§‹åŒ–æ•æ„Ÿè¯è¿‡æ»¤å™¨
        :param word_file_path: æ•æ„Ÿè¯æ–‡ä»¶è·¯å¾„
        """
        self.automaton = ahocorasick.Automaton()
        self.total_matches = 0
        self._load_sensitive_words(word_file_path)
    
    def _load_sensitive_words(self, file_path):
        """ä»æ–‡ä»¶åŠ è½½æ•æ„Ÿè¯"""
        if not os.path.exists(file_path):
            # åˆ›å»ºé»˜è®¤çš„æ•æ„Ÿè¯æ–‡ä»¶ç¤ºä¾‹
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write("# æ•æ„Ÿè¯åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªæˆ–å¤šä¸ªç”¨ç©ºæ ¼åˆ†éš”ï¼‰\n")
                f.write("æ•æ„Ÿè¯1 æ•æ„Ÿè¯2 æ•æ„Ÿè¯3\n")
                f.write("æµ‹è¯•æ•æ„Ÿè¯\n")
                f.write("è¿è§„å†…å®¹\n")
            print(f"âš ï¸  æ•æ„Ÿè¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: {file_path}")
        
        sensitive_words = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):  # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                    words = line.split()
                    sensitive_words.extend(words)
        
        for word in sensitive_words:
            if word:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                self.automaton.add_word(word, word)
        
        self.automaton.make_automaton()
        print(f"âœ… åŠ è½½ {len(sensitive_words)} ä¸ªæ•æ„Ÿè¯: {sensitive_words[:10]}{'...' if len(sensitive_words) > 10 else ''}")
        print(f"âœ… å½“å‰æ•æ„Ÿåº¦é˜ˆå€¼: {get_sensitivity_threshold()}")
    
    def count_matches(self, text: str) -> int:
        """
        ç»Ÿè®¡æ–‡æœ¬ä¸­æ•æ„Ÿè¯åŒ¹é…æ¬¡æ•°
        :param text: å¾…æ£€æµ‹æ–‡æœ¬
        :return: åŒ¹é…æ¬¡æ•°
        """
        if not text or not isinstance(text, str):
            return 0
        
        count = 0
        for end_index, original_value in self.automaton.iter(text):
            count += 1
            # è°ƒè¯•è¾“å‡º
            start_index = end_index - len(original_value) + 1
            matched_word = text[start_index:end_index + 1]
            print(f"ğŸ” åŒ¹é…æ•æ„Ÿè¯: '{matched_word}' at position {start_index}-{end_index}")
            print(f"   ä¸Šä¸‹æ–‡: ...{text[max(0, start_index-10):start_index]}ã€{matched_word}ã€‘{text[end_index+1:min(len(text), end_index+11)]}...")
        
        if count > 0:
            print(f"âœ… åœ¨æ–‡æœ¬ä¸­å‘ç° {count} ä¸ªæ•æ„Ÿè¯åŒ¹é…")
        
        return count
    
    def filter_content(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å®¡æŸ¥æ•°æ®å†…å®¹å¹¶è¿”å›åŒ¹é…æ¬¡æ•°
        :param data: åŒ…å«å„ç§å†…å®¹çš„æ•°æ®å­—å…¸
        :return: åŒ…å«å®¡æŸ¥ç»“æœçš„æ•°æ®å­—å…¸
        """
        total_matches = 0
        content_fields = self._get_content_fields(data)
    
        print(f"ğŸ” å®¡æŸ¥æ•°æ®ï¼Œæ‰¾åˆ° {len(content_fields)} ä¸ªå†…å®¹å­—æ®µ")
    
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹å­—æ®µï¼Œæ˜¾ç¤ºæ•°æ®ç»“æ„ç”¨äºè°ƒè¯•
        if len(content_fields) == 0:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å†…å®¹å­—æ®µï¼Œæ•°æ®ç»“æ„å¯èƒ½æ˜¯:")
            print(f"   æ•°æ®ç±»å‹: {type(data)}")
            if isinstance(data, dict):
                print(f"   å­—å…¸é”®: {list(data.keys())}")
                if "extracted_content" in data:
                    print(f"   extracted_contentç±»å‹: {type(data['extracted_content'])}")
                    if isinstance(data['extracted_content'], dict):
                        print(f"   extracted_contenté”®: {list(data['extracted_content'].keys())}")
    
        # æ‰“å°æ‰€æœ‰æ‰¾åˆ°çš„å†…å®¹å­—æ®µ
        for field_path, content in content_fields:
            print(f"   ğŸ“ å­—æ®µ: {field_path}")
            print(f"     å†…å®¹: {content[:100]}...")
    
        # ç»Ÿè®¡æ‰€æœ‰å†…å®¹å­—æ®µçš„åŒ¹é…æ¬¡æ•°
        for field_path, content in content_fields:
            if isinstance(content, str):
                print(f"ğŸ” æ£€æŸ¥å­—æ®µ: {field_path}")
                matches = self.count_matches(content)
                total_matches += matches
                if matches > 0:
                    self._add_match_info(data, field_path, matches, content)
                    print(f"ğŸš¨ å­—æ®µ {field_path} å‘ç° {matches} ä¸ªæ•æ„Ÿè¯åŒ¹é…")
    
        # æ·»åŠ å®¡æŸ¥ç»“æœä¿¡æ¯
        threshold = get_sensitivity_threshold()
        data["sensitive_check"] = {
            "total_matches": total_matches,
            "is_sensitive": total_matches >= threshold,
            "checked_fields": len(content_fields),
            "threshold": threshold
        }
    
        self.total_matches += total_matches
    
        return data
    
    def _get_content_fields(self, data: Dict, base_path: str = "") -> List:
        """
        é€’å½’è·å–æ‰€æœ‰å¯èƒ½åŒ…å«æ–‡æœ¬å†…å®¹çš„å­—æ®µ
        :param data: æ•°æ®å­—å…¸
        :param base_path: å½“å‰è·¯å¾„
        :return: å†…å®¹å­—æ®µåˆ—è¡¨ [(è·¯å¾„, å†…å®¹)]
        """
        content_fields = []
    
        # å®šä¹‰éœ€è¦æ£€æŸ¥çš„å†…å®¹å­—æ®µæ¨¡å¼
        content_patterns = {
            'title', 'description', 'content', 'text', 
            'message', 'name', 'query', 'comment',
            'caption', 'transcript', 'summary', 'label',
            'snippet', 'display', 'simpleText', 'accessibility',
            'shortDescription', 'ownerChannelName', 'qualityLabel',
            'contextParams', 'contentLength'  # æ·»åŠ YouTubeç‰¹æœ‰å­—æ®µ
        }
    
        # é¦–å…ˆæ£€æŸ¥ extracted_content å­—æ®µï¼ˆæ¥è‡ªyoutube_proxyçš„æå–ç»“æœï¼‰
        if "extracted_content" in data and isinstance(data["extracted_content"], dict):
            for key, values in data["extracted_content"].items():
                if any(pattern in key.lower() for pattern in content_patterns):
                    if isinstance(values, list):
                        for i, value in enumerate(values):
                            if isinstance(value, str) and value.strip():
                                field_path = f"extracted_content.{key}[{i}]"
                                content_fields.append((field_path, value))
                                print(f"ğŸ” æ‰¾åˆ°å†…å®¹å­—æ®µ: {field_path} = {value[:50]}...")
    
        # ç„¶åé€’å½’æ£€æŸ¥å…¶ä»–åµŒå¥—ç»“æ„
        if isinstance(data, dict):
            for key, value in data.items():
                current_path = f"{base_path}.{key}" if base_path else key
            
                # è·³è¿‡å·²ç»å¤„ç†è¿‡çš„extracted_content
                if key == "extracted_content":
                    continue
                
                # å¦‚æœå­—æ®µååŒ…å«å†…å®¹å…³é”®è¯ï¼Œä¸”å€¼æ˜¯å­—ç¬¦ä¸²
                if (isinstance(key, str) and 
                    any(pattern in key.lower() for pattern in content_patterns) and 
                    isinstance(value, str) and value.strip()):
                    content_fields.append((current_path, value))
                    print(f"ğŸ” æ‰¾åˆ°å†…å®¹å­—æ®µ: {current_path} = {value[:50]}...")
            
                # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œé€’å½’æ£€æŸ¥
                if isinstance(value, (dict, list)):
                    content_fields.extend(self._get_content_fields(value, current_path))
    
        elif isinstance(data, list):
            for i, item in enumerate(data):
                current_path = f"{base_path}[{i}]" if base_path else f"[{i}]"
                if isinstance(item, (dict, list)):
                    content_fields.extend(self._get_content_fields(item, current_path))
    
        return content_fields
    
    def _add_match_info(self, data: Dict, field_path: str, matches: int, content: str):
        """
        æ·»åŠ åŒ¹é…ä¿¡æ¯åˆ°æ•°æ®ä¸­
        :param data: åŸå§‹æ•°æ®
        :param field_path: å­—æ®µè·¯å¾„
        :param matches: åŒ¹é…æ¬¡æ•°
        :param content: åŸå§‹å†…å®¹
        """
        if "sensitive_matches" not in data:
            data["sensitive_matches"] = []
        
        data["sensitive_matches"].append({
            "field": field_path,
            "matches": matches,
            "content_sample": content[:100] + "..." if len(content) > 100 else content
        })
    
    def replace_sensitive_content(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ›¿æ¢æ•æ„Ÿå†…å®¹ä¸ºå ä½ç¬¦ï¼ˆä¿æŒæ•°æ®ç»“æ„ä¸å˜ï¼‰
        :param data: åŸå§‹æ•°æ®
        :return: æ›¿æ¢åçš„æ•°æ®
        """
        def _recursive_replace(obj, path=""):
            if isinstance(obj, dict):
                result = {}
                for key, value in obj.items():
                    current_path = f"{path}.{key}" if path else key
                    result[key] = _recursive_replace(value, current_path)
                return result
            elif isinstance(obj, list):
                return [_recursive_replace(item, f"{path}[{i}]") for i, item in enumerate(obj)]
            elif isinstance(obj, str):
                # æ£€æŸ¥å­—æ®µè·¯å¾„æ˜¯å¦åŒ…å«å†…å®¹å…³é”®è¯
                content_patterns = {'title', 'description', 'content', 'text', 'message', 'name'}
                if any(pattern in path.lower() for pattern in content_patterns):
                    return "âš ï¸ è¯¥å†…å®¹å·²è¢«è¿‡æ»¤"
                return obj
            else:
                return obj
        
        return _recursive_replace(data)

# åˆ›å»ºå…¨å±€è¿‡æ»¤å™¨å®ä¾‹
sensitive_filter = SensitiveFilter()

def check_and_filter_data(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    å®¡æŸ¥å¹¶è¿‡æ»¤æ•°æ®çš„å…¬å…±æ¥å£
    :param data: è¾“å…¥æ•°æ®
    :return: å¤„ç†åçš„æ•°æ®
    """
    # é¦–å…ˆè¿›è¡Œæ•æ„Ÿè¯æ£€æµ‹
    checked_data = sensitive_filter.filter_content(data)
    
    # å¦‚æœæ•æ„ŸåŒ¹é…æ¬¡æ•° >= é˜ˆå€¼ï¼Œæ›¿æ¢æ‰€æœ‰å†…å®¹
    threshold = get_sensitivity_threshold()
    if checked_data.get("sensitive_check", {}).get("is_sensitive", False):
        print(f"ğŸš¨ æ£€æµ‹åˆ°æ•æ„Ÿå†…å®¹ï¼ŒåŒ¹é…æ¬¡æ•°: {checked_data['sensitive_check']['total_matches']} (é˜ˆå€¼: {threshold})")
        filtered_data = sensitive_filter.replace_sensitive_content(checked_data)
        filtered_data["sensitive_check"]["action_taken"] = "content_replaced"
        return filtered_data
    
    checked_data["sensitive_check"]["action_taken"] = "passed"
    return checked_data

def get_filter_stats() -> Dict[str, Any]:
    """è·å–è¿‡æ»¤å™¨ç»Ÿè®¡ä¿¡æ¯"""
    return {
        "total_matches": sensitive_filter.total_matches,
        "automaton_size": len(sensitive_filter.automaton),
        "sensitivity_threshold": get_sensitivity_threshold()
    }

# æµ‹è¯•å‡½æ•°
def test_sensitive_filter():
    """æµ‹è¯•æ•æ„Ÿè¯æ£€æµ‹åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ•æ„Ÿè¯æ£€æµ‹åŠŸèƒ½...")
    
    # æµ‹è¯•ä¸åŒé˜ˆå€¼
    test_thresholds = [1, 3, 5]
    
    for threshold in test_thresholds:
        print(f"\nğŸ”§ æµ‹è¯•é˜ˆå€¼: {threshold}")
        set_sensitivity_threshold(threshold)
        
        filter = SensitiveFilter("sensitive_words.txt")
        
        # æµ‹è¯•æ–‡æœ¬
        test_text = "è¿™æ˜¯ä¸€ä¸ªåŒ…å«æ•æ„Ÿè¯å’Œæµ‹è¯•æ•æ„Ÿè¯çš„æ–‡æœ¬"
        
        matches = filter.count_matches(test_text)
        print(f"æ–‡æœ¬: '{test_text}' -> åŒ¹é…æ¬¡æ•°: {matches}")
        print(f"æ˜¯å¦æ•æ„Ÿ: {matches >= threshold}")

def test_with_actual_content():
    """ä½¿ç”¨å®é™…å†…å®¹æµ‹è¯•"""
    print("ğŸ§ª ä½¿ç”¨å®é™…YouTubeå†…å®¹æµ‹è¯•æ•æ„Ÿè¯æ£€æµ‹...")
    
    # è®¾ç½®é˜ˆå€¼ä¸º1
    set_sensitivity_threshold(1)
    
    filter = SensitiveFilter("sensitive_words.txt")
    
    # æ¨¡æ‹ŸYouTubeå“åº”æ•°æ®
    test_data = {
        "title": "ä¹ä¸‰é˜…å…µèƒŒåï¼Œè¿™4ä¸ªæ•…äº‹è¯»æ‡‚ä¸­å›½å†›äºº | CCTVã€Œé¢å¯¹é¢ã€",
        "shortDescription": "2025å¹´9æœˆ3æ—¥ï¼Œçºªå¿µä¸­å›½äººæ°‘æŠ—æ—¥æˆ˜äº‰æš¨ä¸–ç•Œåæ³•è¥¿æ–¯æˆ˜äº‰èƒœåˆ©80å‘¨å¹´é˜…å…µå¼åœ¨å¤©å®‰é—¨å¹¿åœºéš†é‡ä¸¾è¡Œã€‚",
        "simpleText": "ä¹ä¸‰é˜…å…µèƒŒåï¼Œè¿™4ä¸ªæ•…äº‹è¯»æ‡‚ä¸­å›½å†›äºº | CCTVã€Œé¢å¯¹é¢ã€",
        "ownerChannelName": "CCTVä¸­å›½ä¸­å¤®ç”µè§†å°"
    }
    
    print("ğŸ“‹ æµ‹è¯•æ•°æ®:")
    for key, value in test_data.items():
        print(f"   {key}: {value}")
    
    # æµ‹è¯•å•ä¸ªå­—æ®µ
    print("\nğŸ” æµ‹è¯•å•ä¸ªå­—æ®µ:")
    content = test_data["shortDescription"]
    matches = filter.count_matches(content)
    print(f"   shortDescription åŒ¹é…æ¬¡æ•°: {matches}")
    
    # æµ‹è¯•å®Œæ•´æ•°æ®ç»“æ„
    print("\nğŸ” æµ‹è¯•å®Œæ•´æ•°æ®ç»“æ„:")
    result = filter.filter_content(test_data)
    print(f"   æ€»åŒ¹é…æ¬¡æ•°: {result.get('sensitive_check', {}).get('total_matches', 0)}")
    print(f"   æ˜¯å¦æ•æ„Ÿ: {result.get('sensitive_check', {}).get('is_sensitive', False)}")
    
    if result.get('sensitive_matches'):
        for match in result['sensitive_matches']:
            print(f"   ğŸš¨ åŒ¹é…: {match['field']} - {match['matches']}æ¬¡")

if __name__ == "__main__":
    test_with_actual_content()